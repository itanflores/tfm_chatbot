import streamlit as st
import pandas as pd
import numpy as np
from io import StringIO
from google.cloud import storage
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from chatterbot import ChatBot
from chatterbot.trainers import ListTrainer

# ğŸ“Œ ConfiguraciÃ³n del Cliente de Google Cloud Storage
BUCKET_NAME = "monitoreo_gcp_bucket"
ARCHIVO_DATOS = "dataset_monitoreo_servers.csv"

# Diccionario con los nombres de los datasets procesados para cada modelo
ARCHIVOS_PROCESADOS = {
    "Ãrbol de DecisiÃ³n": "dataset_procesado_arbol_decision.csv",
    "RegresiÃ³n LogÃ­stica": "dataset_procesado_regresion_logistica.csv",
    "Random Forest": "dataset_procesado_random_forest.csv"
}

# Inicializar cliente de Google Cloud Storage
storage_client = storage.Client()
bucket = storage_client.bucket(BUCKET_NAME)

# ğŸ“Œ FunciÃ³n para cargar los datos desde GCP Storage
@st.cache_data
def cargar_datos():
    try:
        blob = bucket.blob(ARCHIVO_DATOS)
        contenido = blob.download_as_text()
        df = pd.read_csv(StringIO(contenido))
        return df
    except Exception as e:
        st.error(f"âŒ Error al descargar el archivo desde GCP: {e}")
        return None

df = cargar_datos()
if df is None:
    st.stop()

# ğŸ“Œ FunciÃ³n para procesar los datos (ahora se hace por modelo)
def procesar_datos(df, modelo):
    df_procesado = df.copy()

    # Convertir fecha
    df_procesado["Fecha"] = pd.to_datetime(df_procesado["Fecha"], errors="coerce")

    # Eliminar duplicados y valores nulos
    df_procesado.drop_duplicates(inplace=True)
    df_procesado.dropna(inplace=True)

    # CodificaciÃ³n ordinal para "Estado del Sistema"
    estado_mapping = {"Inactivo": 0, "Normal": 1, "Advertencia": 2, "CrÃ­tico": 3}
    df_procesado["Estado del Sistema Codificado"] = df_procesado["Estado del Sistema"].map(estado_mapping)

    # CodificaciÃ³n one-hot para "Tipo de Servidor"
    df_procesado = pd.get_dummies(df_procesado, columns=["Tipo de Servidor"], prefix="Servidor", drop_first=True)

    # NormalizaciÃ³n de mÃ©tricas continuas (segÃºn modelo)
    scaler = MinMaxScaler()
    metricas_continuas = ["Uso CPU (%)", "Temperatura (Â°C)", "Carga de Red (MB/s)", "Latencia Red (ms)"]

    # Aplicamos escalado diferente si es RegresiÃ³n LogÃ­stica
    if modelo == "RegresiÃ³n LogÃ­stica":
        df_procesado[metricas_continuas] = (df_procesado[metricas_continuas] - df_procesado[metricas_continuas].mean()) / df_procesado[metricas_continuas].std()
    else:
        df_procesado[metricas_continuas] = scaler.fit_transform(df_procesado[metricas_continuas])

    return df_procesado

# ğŸ“Œ Estado de datos procesados
if "datos_procesados" not in st.session_state:
    st.session_state["datos_procesados"] = {}

# ğŸ“Œ SECCIÃ“N: COMPARACIÃ“N DE MODELOS
st.header("ğŸ“Š ComparaciÃ³n de Modelos de ClasificaciÃ³n")

tab1, tab2, tab3, tab4 = st.tabs(["ğŸŒ³ Ãrbol de DecisiÃ³n", "ğŸ“ˆ RegresiÃ³n LogÃ­stica", "ğŸŒ² Random Forest", "ğŸ¤– ChatBot de Soporte"])

for tab, modelo in zip([tab1, tab2, tab3], ARCHIVOS_PROCESADOS.keys()):
    with tab:
        st.subheader(f"{modelo}")

        if st.button(f"âš™ï¸ Procesar Datos para {modelo}"):
            df_procesado = procesar_datos(df, modelo)
            st.session_state["datos_procesados"][modelo] = df_procesado
            st.success(f"âœ… Datos procesados correctamente para {modelo}.")

        # ğŸ“Œ BotÃ³n de exportaciÃ³n de datos (solo aparece si los datos fueron procesados)
        if modelo in st.session_state["datos_procesados"]:
            def exportar_datos():
                try:
                    df_procesado = st.session_state["datos_procesados"][modelo]
                    archivo_salida = ARCHIVOS_PROCESADOS[modelo]
                    blob_procesado = bucket.blob(archivo_salida)
                    blob_procesado.upload_from_string(df_procesado.to_csv(index=False), content_type="text/csv")
                    st.success(f"âœ… Datos procesados de {modelo} exportados a {BUCKET_NAME}/{archivo_salida}")
                except Exception as e:
                    st.error(f"âŒ Error al exportar datos a GCP: {e}")

            if st.button(f"ğŸ“¤ Guardar Datos de {modelo} en GCP"):
                exportar_datos()

# ğŸ“Œ SECCIÃ“N: Chatbot de Soporte
with tab4:
    st.subheader("ğŸ¤– ChatBot de Soporte para Infraestructura TI")
    st.write("Puedes preguntarme sobre el monitoreo de servidores, modelos de clasificaciÃ³n y mÃ¡s.")

    # ğŸ“Œ Inicializar el chatbot
    @st.cache_resource
    def iniciar_chatbot():
        chatbot = ChatBot("Soporte TI")
        trainer = ListTrainer(chatbot)

        preguntas_respuestas = [
            "Â¿QuÃ© modelos de clasificaciÃ³n estÃ¡n disponibles?",
            "Los modelos disponibles son: Ãrbol de DecisiÃ³n, RegresiÃ³n LogÃ­stica y Random Forest.",
            "Â¿DÃ³nde se guardan los datos procesados?",
            "Los datos procesados se guardan en el bucket de GCP: monitoreo_gcp_bucket.",
            "Â¿CÃ³mo exporto datos a S3?",
            "Puedes exportar los datos usando el botÃ³n 'Exportar a S3' despuÃ©s de procesarlos.",
            "Â¿CuÃ¡l es la precisiÃ³n del modelo Random Forest?",
            "La precisiÃ³n del modelo Random Forest es aproximadamente del 95%.",
            "Â¿CuÃ¡ntos registros hay en el dataset?",
            f"El dataset tiene {len(df)} registros.",
        ]

        trainer.train(preguntas_respuestas)
        return chatbot

    chatbot = iniciar_chatbot()

    # ğŸ“Œ Interfaz del Chatbot
    user_input = st.text_input("ğŸ’¬ Escribe tu pregunta:", "")
    if st.button("Enviar"):
        if user_input:
            respuesta = chatbot.get_response(user_input)
            st.text_area("ğŸ¤– Respuesta:", value=str(respuesta), height=100, max_chars=None)
        else:
            st.warning("âš ï¸ Por favor, ingresa una pregunta.")

